{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reedmershon/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2881: FutureWarning: \n",
      "mpl_style had been deprecated and will be removed in a future version.\n",
      "Use `matplotlib.pyplot.style.use` instead.\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as PCA\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.mpl_style', 'default')\n",
    "\n",
    "dfPolluters = pd.read_csv('honeypot_data/content_polluters_tweets.txt', sep='\\t', names=['UserID', 'TweetID', 'Tweet', 'CreatedAt'], parse_dates=['CreatedAt'])\n",
    "polluters = pd.read_csv('honeypot_data/content_polluters.txt', sep='\\t', names=['UserID', 'CreatedAt', 'CollectedAt', 'NumberOfFollowings', 'NumberOfFollowers', 'NumberOfTweets', 'LengthOfScreenName', 'LengthOfDescriptionInUserProfile'], parse_dates=['CreatedAt', 'CollectedAt'])\n",
    "pol_dow = pd.read_csv('created_data/polDOWeek.csv', index_col=0)\n",
    "\n",
    "polluters = polluters.set_index('UserID')\n",
    "\n",
    "dfLegit = pd.read_csv('honeypot_data/legitimate_users_tweets.txt', sep='\\t', names=['UserID', 'TweetID', 'Tweet', 'CreatedAt'], parse_dates=['CreatedAt'])\n",
    "legit = pd.read_csv('honeypot_data/legitimate_users.txt', sep='\\t', names=['UserID', 'CreatedAt', 'CollectedAt', 'NumberOfFollowings', 'NumberOfFollowers', 'NumberOfTweets', 'LengthOfScreenName', 'LengthOfDescriptionInUserProfile'], parse_dates=['CreatedAt', 'CollectedAt'])\n",
    "leg_dow = pd.read_csv('created_data/legDOWeek.csv', index_col=0)\n",
    "\n",
    "legit = legit.set_index('UserID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets with links freq\n",
    "Tweets with unique links freq\n",
    "Tweets with username freq\n",
    "Tweets with unique username freq\n",
    "\n",
    "Total Tweets\n",
    "\n",
    "Use the sentiment analysis\n",
    "\n",
    "Tweets perDay\n",
    "\n",
    "\n",
    "Combine all of these into one single DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polluters = polluters.drop(['CreatedAt', 'CollectedAt', 'NumberOfFollowings', 'NumberOfFollowers', 'LengthOfScreenName', 'LengthOfDescriptionInUserProfile'], axis=1)\n",
    "legit = legit.drop(['CreatedAt', 'CollectedAt', 'NumberOfFollowings', 'NumberOfFollowers', 'LengthOfScreenName', 'LengthOfDescriptionInUserProfile'], axis=1)\n",
    "\n",
    "polluters = pd.concat([polluters, pol_dow], axis=1, join_axes=[polluters.index])\n",
    "legit = pd.concat([legit, leg_dow], axis=1, join_axes=[legit.index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will drop all the accounts with less than 20 tweets collected. We did this already for the day of week parsing, but this will be appended to other accounts so we can just drop them now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polluters = polluters.dropna()\n",
    "legit = legit.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data for Usernames and Links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "username = r'(?<=@)\\w+'\n",
    "\n",
    "def extractContent(users, tweets):\n",
    "    \n",
    "    usersFreq = {}\n",
    "    \n",
    "    for index, row in users.iterrows():\n",
    "        \n",
    "        totalTweets = row[0]\n",
    "        urls = {}\n",
    "        usernames = {}\n",
    "        userTweetFreq = {'TotalLinks': 0, 'UniqueLinks': 0, 'TotalNames': 0, 'UniqueNames': 0}\n",
    "        \n",
    "        userData = tweets.loc[tweets['UserID'] == index]\n",
    "        \n",
    "        for tweet in userData['Tweet']:\n",
    "            tweet = str(tweet)\n",
    "            foundURLs = re.findall(url, tweet)\n",
    "            foundNames = re.findall(username, tweet)\n",
    "\n",
    "            for u in foundURLs:\n",
    "                if u in urls:\n",
    "                    urls[u] += 1\n",
    "                else:\n",
    "                    urls[u] = 1\n",
    "                    \n",
    "            for n in foundNames:\n",
    "                if n in usernames:\n",
    "                    usernames[n] += 1\n",
    "                else:\n",
    "                    usernames[n] = 1\n",
    "                    \n",
    "        totalUrls = sum(urls.values())\n",
    "        totalNames = sum(usernames.values())\n",
    "        uniqueUrls = len(urls)\n",
    "        uniqueNames = len(usernames)\n",
    "        \n",
    "#         if totalNames >0:\n",
    "#             print(usernames.keys())\n",
    "#             print(usernames.values())\n",
    "        \n",
    "        freqUrl = totalUrls/totalTweets\n",
    "        freqUniqueUrl = uniqueUrls/totalTweets\n",
    "        freqName = totalNames/totalTweets\n",
    "        freqUniqueName = uniqueNames/totalTweets\n",
    "        \n",
    "        userTweetFreq['TotalLinks'] = totalUrls\n",
    "        userTweetFreq['FreqLinks'] = freqUrl\n",
    "        userTweetFreq['UniqueLinks'] = freqUniqueUrl\n",
    "        userTweetFreq['TotalNames'] = totalNames\n",
    "        userTweetFreq['FreqNames'] = freqName\n",
    "        userTweetFreq['UniqueNames'] = freqUniqueName\n",
    "    \n",
    "        usersFreq[index] = userTweetFreq\n",
    "                    \n",
    "    return pd.DataFrame.from_dict(usersFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polFreq = extractContent(polluters, dfPolluters).T\n",
    "legFreq = extractContent(legit, dfLegit).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polluters = pd.concat([polluters, polFreq], axis=1, join_axes=[polluters.index])\n",
    "legit = pd.concat([legit, legFreq], axis=1, join_axes=[legit.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumberOfTweets</th>\n",
       "      <th>Mon.</th>\n",
       "      <th>Tues.</th>\n",
       "      <th>Wed.</th>\n",
       "      <th>Thur.</th>\n",
       "      <th>Fri.</th>\n",
       "      <th>Sat.</th>\n",
       "      <th>Sun.</th>\n",
       "      <th>FreqLinks</th>\n",
       "      <th>FreqNames</th>\n",
       "      <th>TotalLinks</th>\n",
       "      <th>TotalNames</th>\n",
       "      <th>UniqueLinks</th>\n",
       "      <th>UniqueNames</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>861</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.163763</td>\n",
       "      <td>0.077816</td>\n",
       "      <td>141.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.148664</td>\n",
       "      <td>0.051103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10836</th>\n",
       "      <td>226</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.146465</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.238938</td>\n",
       "      <td>130.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.446903</td>\n",
       "      <td>0.057522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NumberOfTweets      Mon.     Tues.      Wed.     Thur.      Fri.  \\\n",
       "UserID                                                                     \n",
       "6301               861  0.155000  0.330000  0.150000  0.145000  0.095000   \n",
       "10836              226  0.106061  0.171717  0.151515  0.146465  0.257576   \n",
       "\n",
       "            Sat.      Sun.  FreqLinks  FreqNames  TotalLinks  TotalNames  \\\n",
       "UserID                                                                     \n",
       "6301    0.030000  0.095000   0.163763   0.077816       141.0        67.0   \n",
       "10836   0.055556  0.111111   0.575221   0.238938       130.0        54.0   \n",
       "\n",
       "        UniqueLinks  UniqueNames  \n",
       "UserID                            \n",
       "6301       0.148664     0.051103  \n",
       "10836      0.446903     0.057522  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polluters[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polluters.to_csv('created_data/userContentPol.csv', encoding='utf-8')\n",
    "legit.to_csv('created_data/userContentLeg.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18195\n"
     ]
    }
   ],
   "source": [
    "print(len(legFreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNClass\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier #Neural Network\n",
    "\n",
    "#needed for the plots\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sys\n",
    "\n",
    "#Redoing these functions to take in the fold number and then automate the list generation\n",
    "#default fold number is 10\n",
    "\n",
    "#take the average of the above list, then look for the best feature set\n",
    "def getROC(df1, df2, fold=10):\n",
    "    #Need to possibly take in a list then append the two data frames\n",
    "    #then remove NaN\n",
    "    \n",
    "    if type(df1) is list:\n",
    "        df1 = pd.concat(df1, axis=1, join_axes=[df1[0].index])\n",
    "        df1 = df1.dropna(axis=0)\n",
    "    if type(df2) is list:\n",
    "        df2 = pd.concat(df2, axis=1, join_axes=[df2[0].index])\n",
    "        df1 = df.dropna(axis=0)\n",
    "    \n",
    "    #section one, prepare data for the learning algorithms\n",
    "    #divide into equal sections, take 10 total sets from each dataset\n",
    "    #give proper classifications for each\n",
    "    foldData = {}\n",
    "    df1Segs = []\n",
    "    df2Segs = []\n",
    "    #add in a parameter for the number of folds to use\n",
    "    segDF1 = int(len(df1)/fold)\n",
    "    segDF2 = int(len(df2)/fold)\n",
    "    \n",
    "    for i in range(fold):\n",
    "        foldData[i] = []\n",
    "        df1Segs.append(i*segDF1)\n",
    "        df2Segs.append(i*segDF2)\n",
    "\n",
    "    for i in range(len(df1Segs)):\n",
    "        #get the fold to fit on\n",
    "        try:\n",
    "            fitDF1 = df1[df1Segs[i]:df1Segs[i+1]]\n",
    "            fitDF2 = df2[df2Segs[i]:df2Segs[i+1]]\n",
    "            \n",
    "            predDF1 = pd.concat([df1[:df1Segs[i]], df1[df1Segs[i+1]:]])\n",
    "            predDF2 = pd.concat([df2[:df2Segs[i]], df2[df2Segs[i+1]:]])\n",
    "        except IndexError: #Only for the last fold\n",
    "            fitDF1 = df1[df1Segs[i]:]\n",
    "            fitDF2 = df2[df2Segs[i]:]\n",
    "            \n",
    "            predDF1 = df1[:df1Segs[i]]\n",
    "            predDF2 = df2[:df2Segs[i]]\n",
    "            \n",
    "        #use the DF to get the data\n",
    "        fitData, fitCls = machLearnData(fitDF1, fitDF2)\n",
    "        preData, preCls = machLearnData(predDF1, predDF2)\n",
    "    \n",
    "        #second one apply all algorithms to it\n",
    "        #define the algorithms\n",
    "        neigh = KNClass(n_neighbors=21)\n",
    "        gnb = GaussianNB()\n",
    "        dtc = DecisionTreeClassifier(random_state=0)\n",
    "        vec = svm.SVC(probability=True)\n",
    "        neur = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "        Algs = [neigh, gnb, dtc, vec, neur]#can add more later, also the order they are added to the dictionary\n",
    "        \n",
    "        for alg in Algs:\n",
    "            alg.fit(fitData, fitCls)\n",
    "            finCls = alg.predict_proba(preData)\n",
    "            \n",
    "            fpr, tpr, thresholds, roc_auc = probaTable(finCls)\n",
    "            algList = [fpr, tpr, thresholds, roc_auc]\n",
    "            \n",
    "            foldData[i].append(algList)\n",
    "        \n",
    "    \n",
    "    return foldData\n",
    "    \n",
    "    #get out the ROC curvs and AUC scores for each\n",
    "    \n",
    "    \n",
    "#this will give the proper classifications\n",
    "#dframe1 should be the positive df\n",
    "def machLearnData(dframe1, dframe2, rng=0):\n",
    "    data = []\n",
    "    #0 = polluters\n",
    "    #1 = legit\n",
    "    classify = []\n",
    "    for index, row in dframe1.iterrows():\n",
    "        userList = []\n",
    "        for i in range(len(row)):\n",
    "            userList.append(row[i])\n",
    "            \n",
    "        data.append(userList)\n",
    "        classify.append(0)\n",
    "        \n",
    "    for index, row in dframe2.iterrows():\n",
    "        userList = []\n",
    "        for i in range(len(row)):\n",
    "            userList.append(row[i])\n",
    "\n",
    "        data.append(userList)\n",
    "        classify.append(1)\n",
    "        \n",
    "    return data, classify\n",
    "\n",
    "def probaTable(fCls, posL=0, posCut=0.5):\n",
    "    clsArray = []\n",
    "    actualCls = []\n",
    "    \n",
    "    pos = []\n",
    "    \n",
    "    count = 0\n",
    "    for line in fCls:\n",
    "        if line[0] >= posCut:\n",
    "            clsArray.append(0)\n",
    "        else:\n",
    "            clsArray.append(1)\n",
    "        pos.append(line[0])\n",
    "            \n",
    "        if count < len(fCls)/2:\n",
    "            actualCls.append(0)\n",
    "            count += 1\n",
    "        else:\n",
    "            actualCls.append(1)\n",
    "            \n",
    "    #print(len(actualCls), len(fCls))\n",
    "            \n",
    "    #classTuple = zip(actualCls, fCls)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(actualCls, pos, pos_label=posL)\n",
    "    \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    #return {'polluter': [truePos, falseNeg], 'legit': [falsePos, trueNeg]}, acc, precision, recall, f1, fpr\n",
    "    \n",
    "    return fpr, tpr, thresholds, roc_auc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-72bcaadcd6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrocUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetROC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolluters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-1fc1ee0aebf3>\u001b[0m in \u001b[0;36mgetROC\u001b[0;34m(df1, df2, fold)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0malg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAlgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitCls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mfinCls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/reedmershon/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/reedmershon/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/reedmershon/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/reedmershon/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "rocUC = getROC(polluters, legit, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
